{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab0bf265-34c7-41b7-a46d-6e7786ef63e6",
   "metadata": {},
   "source": [
    "# Home Credit Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b5b4e1-27e9-4e5b-8bd4-a3b4e0cc3812",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81176a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d9412-7f9a-419a-8744-7e4b5d53ea58",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d73650d-eb09-47c2-935c-ae499700aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_countplot(\n",
    "    df, ax, col, colors, plot_xlabel=\"\", plot_ylabel=\"\", percent=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a vertical countplot with the provided percentages on top.\n",
    "    \"\"\"\n",
    "    counts = df[col].value_counts()\n",
    "    category_labels = [str(cat) for cat in counts.index]\n",
    "    bars = ax.bar(category_labels, counts.values, color=colors)\n",
    "\n",
    "    ax.set_xlabel(plot_xlabel)\n",
    "    ax.set_ylabel(plot_ylabel)\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", length=0)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax.grid(False)\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color(\"black\")\n",
    "\n",
    "    if percent:\n",
    "        total_height = counts.sum()\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(\n",
    "                f\"{height/total_height:.1%}\",\n",
    "                (bar.get_x() + bar.get_width() / 2, height),\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=10,\n",
    "                color=\"black\",\n",
    "                xytext=(0, 2),\n",
    "                textcoords=\"offset points\",\n",
    "            )\n",
    "\n",
    "    ax.set_title(ax.get_title(), y=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14536a49-38a2-4326-a0bc-a37d83209241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_proportion_barchart(\n",
    "    df, ax, col_x, col_y, plot_title=\"\", plot_xlabel=\"\", plot_ylabel=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a bar plot showing the proportion of late payments (TARGET).\n",
    "    \"\"\"\n",
    "    proportion_late = (\n",
    "        df.groupby(col_x, observed=True)[col_y]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .sort_values(by=col_y, ascending=False)\n",
    "    )\n",
    "\n",
    "    bars = proportion_late.plot(\n",
    "        kind=\"bar\", x=col_x, y=col_y, ax=ax, color=custom_palette[1:]\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(plot_xlabel)\n",
    "    ax.set_ylabel(plot_ylabel)\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", length=0)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax.grid(False)\n",
    "    ax.legend_ = None\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color(\"black\")\n",
    "\n",
    "    for bar in bars.patches:\n",
    "        width, height = bar.get_width(), bar.get_height()\n",
    "        x, y = bar.get_xy()\n",
    "        ax.annotate(\n",
    "            f\"{height:.1%}\",\n",
    "            (bar.get_x() + bar.get_width() / 2, height),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "            color=\"black\",\n",
    "            xytext=(0, 2),\n",
    "            textcoords=\"offset points\",\n",
    "        )\n",
    "\n",
    "    if plot_title:\n",
    "        ax.set_title(plot_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93eda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacked_barchart(\n",
    "    df, ax, col_x, col_hue, colors, plot_xlabel=\"\", plot_ylabel=\"\", percent=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a stacked bar plot with the provided percentages on top.\n",
    "    \"\"\"\n",
    "    counts = pd.crosstab(df[col_x], df[col_hue], normalize=\"index\")\n",
    "    bars = counts.plot(kind=\"bar\", stacked=True, color=colors, ax=ax)\n",
    "\n",
    "    ax.set_xlabel(plot_xlabel)\n",
    "    ax.set_ylabel(plot_ylabel)\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", length=0)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax.grid(False)\n",
    "    ax.legend_ = None\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color(\"black\")\n",
    "\n",
    "    if percent:\n",
    "        for bar in bars.patches:\n",
    "            width, height = bar.get_width(), bar.get_height()\n",
    "            x, y = bar.get_xy()\n",
    "            ax.annotate(\n",
    "                f\"{height:.1%}\",\n",
    "                (x + width / 2, y + height / 2),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                fontsize=10,\n",
    "                color=\"white\",\n",
    "                xytext=(0, 2),\n",
    "                textcoords=\"offset points\",\n",
    "            )\n",
    "\n",
    "    ax.set_title(ax.get_title(), y=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f8ccb82-3b34-440a-a17b-47e52bfc7387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_barplot(\n",
    "    df, ax, col, colors, plot_xlabel=\"\", plot_ylabel=\"\", show_values=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a vertical barplot with the provided values.\n",
    "    \"\"\"\n",
    "    bars = ax.barh(df[col], df[\"Correlation\"], color=colors)\n",
    "\n",
    "    ax.set_xlabel(plot_xlabel)\n",
    "    ax.set_ylabel(plot_ylabel)\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", length=0)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.grid(False)\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color(\"black\")\n",
    "\n",
    "    if show_values:\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            ax.annotate(\n",
    "                f\"{width:.2f}\",\n",
    "                (width, bar.get_y() + bar.get_height() / 2),\n",
    "                ha=\"left\",\n",
    "                va=\"center\",\n",
    "                fontsize=10,\n",
    "                color=\"black\",\n",
    "                xytext=(5, 0),  # Offset for the text\n",
    "                textcoords=\"offset points\",\n",
    "            )\n",
    "\n",
    "    ax.set_title(ax.get_title(), y=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e71d1-3568-4679-874b-8dc5be893976",
   "metadata": {},
   "source": [
    "## Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3601310-754f-4ee5-8072-0812b6d4bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Cleans the DataFrame according to specified rules.\"\"\"\n",
    "\n",
    "    replacements = {\n",
    "        \"XNA\": np.nan,\n",
    "        \"XAP\": np.nan,\n",
    "    }\n",
    "\n",
    "    df.replace(replacements, inplace=True)\n",
    "\n",
    "    df[\"DAYS_EMPLOYED\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    df[\"DAYS_LAST_PHONE_CHANGE\"].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    df[\"AGE_YEARS\"] = abs(df[\"DAYS_BIRTH\"]) / 365\n",
    "    df[\"YEARS_EMPLOYED\"] = abs(df[\"DAYS_EMPLOYED\"]) / 365\n",
    "\n",
    "    house_cols_drop = [col for col in df.columns if col.endswith((\"MEDI\", \"MODE\"))]\n",
    "    df.drop(columns=house_cols_drop, inplace=True)\n",
    "\n",
    "    df.drop(columns=[\"DAYS_EMPLOYED\", \"DAYS_BIRTH\"], inplace=True)\n",
    "\n",
    "    df[\"NAME_EDUCATION_TYPE\"].replace(\n",
    "        \"Secondary / secondary special\", \"Secondary\", inplace=True\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75d7ea88-0c9d-4412-957b-590fe6cad2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast(df):\n",
    "    \"\"\"\n",
    "    Downcasts numerical columns to the smallest possible data types (integers and floats)\n",
    "    and converts object columns to categorical to reduce memory usage.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to optimize.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with optimized memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    df_int = df.select_dtypes(include=[\"int\"]).apply(pd.to_numeric, downcast=\"integer\")\n",
    "\n",
    "    df_float = df.select_dtypes(include=[\"float\"]).apply(\n",
    "        pd.to_numeric, downcast=\"float\"\n",
    "    )\n",
    "\n",
    "    df_obj = df.select_dtypes(include=[\"object\"]).astype(\"category\")\n",
    "\n",
    "    df[df_int.columns] = df_int\n",
    "    df[df_float.columns] = df_float\n",
    "    df[df_obj.columns] = df_obj\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f63e4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown_values(column: pd.Series) -> pd.Series:\n",
    "    \"\"\"Replaces specified unknown values with np.nan in a pandas Series.\"\"\"\n",
    "    unknown_values = [\"XNA\", \"Unknown\", \"XAP\"]\n",
    "\n",
    "    column = column.replace(unknown_values, np.nan)\n",
    "\n",
    "    if pd.api.types.is_categorical_dtype(column):\n",
    "        column = column.cat.remove_unused_categories()\n",
    "\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe83ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def years_convert(column: pd.Series) -> pd.Series:\n",
    "    \"\"\"Converts a pandas Series of days to years, handling negative and NaN values.\"\"\"\n",
    "    if not pd.api.types.is_numeric_dtype(column):\n",
    "        raise ValueError(\"Input column must be numeric.\")\n",
    "\n",
    "    years = abs(column) / 365\n",
    "\n",
    "    return years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "212e0306-50b4-48f2-afc9-d492ad3566e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_feature_names(features):\n",
    "    cleaned_features = [\n",
    "        feature.replace(\"/\", \"_\").replace(\" \", \"_\") for feature in features\n",
    "    ]\n",
    "    return cleaned_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346cfd63-ea79-41c2-bfde-ca2640877e28",
   "metadata": {},
   "source": [
    "## Modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea690486-22a4-4738-842f-ed4a0e83d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm_model(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Fit the LightGBM model and evaluate PR AUC and ROC AUC scores.\n",
    "    \"\"\"\n",
    "    categorical_features = X_train.select_dtypes(\n",
    "        include=[\"category\", \"object\"]\n",
    "    ).columns.tolist()\n",
    "\n",
    "    model.fit(X_train, y_train, categorical_feature=categorical_features)\n",
    "\n",
    "    lgb_base_preds_train = model.predict_proba(X_train)[:, 1]\n",
    "    lgb_base_preds_val = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    pr_auc_lgb_base_train = average_precision_score(y_train, lgb_base_preds_train)\n",
    "    pr_auc_lgb_base_val = average_precision_score(y_val, lgb_base_preds_val)\n",
    "    auc_score_train = roc_auc_score(y_train, lgb_base_preds_train)\n",
    "    auc_score_val = roc_auc_score(y_val, lgb_base_preds_val)\n",
    "\n",
    "    print(f\"Base LGBM Train PR AUC Score: {pr_auc_lgb_base_train:.4f}\")\n",
    "    print(f\"Base LGBM Validation PR AUC Score: {pr_auc_lgb_base_val:.4f}\")\n",
    "    print(f\"\\nBase LGBM Train AUC Score: {auc_score_train:.4f}\")\n",
    "    print(f\"Base LGBM Validation AUC Score: {auc_score_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23387b40-d2f7-4f00-8378-c3237b6157b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    num_leaves = trial.suggest_int(\"num_leaves\", 16, 160)\n",
    "    min_child_samples = trial.suggest_int(\"min_child_samples\", 10, 100)\n",
    "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 50)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 0.5)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.7, 1)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0, 10)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 10000)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 10)\n",
    "\n",
    "    lgb = LGBMClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=num_leaves,\n",
    "        min_child_samples=min_child_samples,\n",
    "        min_child_weight=min_child_weight,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        subsample=subsample,\n",
    "        max_depth=max_depth,\n",
    "        reg_lambda=reg_lambda,\n",
    "        reg_alpha=reg_alpha,\n",
    "        n_jobs=16,\n",
    "        is_unbalance=True,\n",
    "        verbose=-1,\n",
    "    )\n",
    "\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        with contextlib.redirect_stdout(fnull):\n",
    "            lgb.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                callbacks=[log_evaluation(period=100), early_stopping(30)],\n",
    "            )\n",
    "\n",
    "    lgb_preds = lgb.predict_proba(X_val)[:, 1]\n",
    "    pr = average_precision_score(y_val, lgb_preds)\n",
    "\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c21878af-91a8-4566-873a-760c656f81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(y_test, y_pred_dummy, final_pred, y_pred_adj):\n",
    "    \"\"\"\n",
    "    Plot normalized confusion matrices for a dummy classifier, LightGBM, and adjusted LightGBM predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    conf_matrix_dummy = confusion_matrix(y_test, y_pred_dummy)\n",
    "    conf_matrix_lgb = confusion_matrix(y_test, final_pred)\n",
    "    conf_matrix_lgb_adj = confusion_matrix(y_test, y_pred_adj)\n",
    "\n",
    "    conf_matrix_dummy = conf_matrix_dummy / conf_matrix_dummy.sum(axis=1, keepdims=True)\n",
    "    conf_matrix_lgb = conf_matrix_lgb / conf_matrix_lgb.sum(axis=1, keepdims=True)\n",
    "    conf_matrix_lgb_adj = conf_matrix_lgb_adj / conf_matrix_lgb_adj.sum(\n",
    "        axis=1, keepdims=True\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(13, 5))\n",
    "\n",
    "    sns.heatmap(\n",
    "        conf_matrix_dummy,\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        fmt=\".2%\",\n",
    "        ax=axes[0],\n",
    "        cbar=False,\n",
    "        xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "        yticklabels=[\"Actual Negative\", \"Actual Positive\"],\n",
    "    )\n",
    "    axes[0].set_title(\"Dummy Classifier\")\n",
    "    axes[0].set_xlabel(\"\")\n",
    "    axes[0].set_ylabel(\"Actual\")\n",
    "\n",
    "    sns.heatmap(\n",
    "        conf_matrix_lgb,\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        fmt=\".2%\",\n",
    "        ax=axes[1],\n",
    "        cbar=False,\n",
    "        xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "        yticklabels=[\"Actual Negative\", \"Actual Positive\"],\n",
    "    )\n",
    "    axes[1].set_title(\"LightGBM\")\n",
    "    axes[1].set_xlabel(\"Predicted\")\n",
    "    axes[1].set_ylabel(\"\")\n",
    "\n",
    "    sns.heatmap(\n",
    "        conf_matrix_lgb_adj,\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        fmt=\".2%\",\n",
    "        ax=axes[2],\n",
    "        cbar=False,\n",
    "        xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "        yticklabels=[\"Actual Negative\", \"Actual Positive\"],\n",
    "    )\n",
    "    axes[2].set_title(\"LightGBM - Adjusted\")\n",
    "    axes[2].set_xlabel(\"\")\n",
    "    axes[2].set_ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d1704b1-2ba3-42ee-9874-7f06fd787fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(\n",
    "    df: pd.DataFrame, cat_features: list, threshold: float = 0.9\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Identify numerical features that are highly correlated based on the training set,\n",
    "    and return the list of features to drop.\n",
    "    \"\"\"\n",
    "    num_df = df.drop(columns=cat_features)\n",
    "\n",
    "    corr_matrix = num_df.corr().abs()\n",
    "\n",
    "    upper_triangle = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_)\n",
    "    )\n",
    "\n",
    "    to_drop = [\n",
    "        column\n",
    "        for column in upper_triangle.columns\n",
    "        if any(upper_triangle[column] > threshold)\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        f\"Number of highly correlated numerical features to be removed: {len(to_drop)}\"\n",
    "    )\n",
    "    return to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f29f8e29-f507-49ea-898b-0dc3616afc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode_auto(\n",
    "    train: pd.DataFrame, val: pd.DataFrame, test: pd.DataFrame, target: pd.Series\n",
    ") -> tuple:\n",
    "    \"\"\"Automatically extract categorical columns and apply target encoding.\"\"\"\n",
    "\n",
    "    categorical_columns = train.select_dtypes(\n",
    "        include=[\"object\", \"category\"]\n",
    "    ).columns.tolist()\n",
    "\n",
    "    encoder = TargetEncoder(cols=categorical_columns)\n",
    "\n",
    "    train_encoded = encoder.fit_transform(train[categorical_columns], target)\n",
    "\n",
    "    val_encoded = encoder.transform(val[categorical_columns])\n",
    "    test_encoded = encoder.transform(test[categorical_columns])\n",
    "\n",
    "    train[categorical_columns] = train_encoded\n",
    "    val[categorical_columns] = val_encoded\n",
    "    test[categorical_columns] = test_encoded\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082065b-2aa8-4e23-b828-36ae9f8becb5",
   "metadata": {},
   "source": [
    "## Statistical Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5c044a4-a676-4cc6-a00c-2fc64e17e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r - 1, k - 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38e127dd-b53b-41c9-b766-a820a99e1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_interval(data):\n",
    "    mean = data.mean()\n",
    "    n = len(data)\n",
    "    standard_error = stats.sem(data)\n",
    "    margin_of_error = 1.96 * standard_error\n",
    "    lower_ci, upper_ci = (mean - margin_of_error), (mean + margin_of_error)\n",
    "    return mean, lower_ci, upper_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873cea74-06d5-4d26-b51b-38957b701ccf",
   "metadata": {},
   "source": [
    "## Feature Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f034b1e8-485c-4bad-9ca7-b7084b94a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_application_features(df):\n",
    "    \"\"\"\n",
    "    Calculates various income and credit ratios and adds them to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to which the calculated ratios will be added.\n",
    "    \"\"\"\n",
    "\n",
    "    # Income and Credit Ratios\n",
    "    df[\"ANNUITY_INCOME_RATIO\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_INCOME_TOTAL\"].replace(\n",
    "        0, np.nan\n",
    "    )\n",
    "    df[\"INCOME_PER_FAMILY_MEMBER\"] = df[\"AMT_INCOME_TOTAL\"] / df[\n",
    "        \"CNT_FAM_MEMBERS\"\n",
    "    ].replace(0, np.nan)\n",
    "    df[\"INCOME_PER_ADULT\"] = df[\"AMT_INCOME_TOTAL\"] / (\n",
    "        df[\"CNT_FAM_MEMBERS\"] - df[\"CNT_CHILDREN\"] + 1\n",
    "    ).replace(0, np.nan)\n",
    "    df[\"CREDIT_PER_FAMILY_MEMBER\"] = df[\"AMT_CREDIT\"] / df[\"CNT_FAM_MEMBERS\"].replace(\n",
    "        0, np.nan\n",
    "    )\n",
    "    df[\"LOAN_TO_INCOME_RATIO\"] = df[\"AMT_CREDIT\"] / df[\"AMT_INCOME_TOTAL\"].replace(\n",
    "        0, np.nan\n",
    "    )\n",
    "\n",
    "    # Credit/Loan Features\n",
    "    df[\"LOAN_TO_VALUE\"] = df[\"AMT_CREDIT\"] / df[\"AMT_GOODS_PRICE\"].replace(0, np.nan)\n",
    "    df[\"ANNUITY_TO_CREDIT\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_CREDIT\"].replace(0, np.nan)\n",
    "    df[\"AMT_DIFF_CREDIT_GOODS\"] = df[\"AMT_CREDIT\"] - df[\"AMT_GOODS_PRICE\"]\n",
    "    df[\"LOAN_TO_GOODS_DIFFERENCE_RATIO\"] = (\n",
    "        df[\"AMT_CREDIT\"] - df[\"AMT_GOODS_PRICE\"]\n",
    "    ) / df[\"AMT_CREDIT\"]\n",
    "    df[\"CREDIT_TO_ANNUITY_RATIO\"] = df[\"AMT_CREDIT\"] / df[\"AMT_ANNUITY\"].replace(\n",
    "        0, np.nan\n",
    "    )\n",
    "\n",
    "    # External Sources\n",
    "    df[\"EXT_SOURCE_MEAN\"] = df[[\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]].mean(\n",
    "        axis=1\n",
    "    )\n",
    "    df[\"EXT_SOURCE_PROD\"] = df[\"EXT_SOURCE_1\"] * df[\"EXT_SOURCE_2\"] * df[\"EXT_SOURCE_3\"]\n",
    "    df[\"EXT_SOURCE_WEIGHTED_AVG\"] = (\n",
    "        0.5 * df[\"EXT_SOURCE_3\"] + 0.3 * df[\"EXT_SOURCE_2\"] + 0.2 * df[\"EXT_SOURCE_1\"]\n",
    "    )\n",
    "    df[\"EXT_SOURCE_STD\"] = df[[\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]].std(\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Employment & Age Ratios\n",
    "    df[\"EMPLOYMENT_AGE_RATIO\"] = df[\"YEARS_EMPLOYED\"] / df[\"AGE_YEARS\"].replace(\n",
    "        0, np.nan\n",
    "    )\n",
    "    df[\"LONG_EMPLOYMENT\"] = np.where(df[\"YEARS_EMPLOYED\"] < 10, 0, 1)\n",
    "\n",
    "    # Family & Occupation\n",
    "    df[\"HAS_CHILDREN\"] = df[\"CNT_CHILDREN\"].map(lambda x: 1 if x > 0 else 0)\n",
    "    df[\"CHILDREN_RATIO\"] = df[\"CNT_CHILDREN\"] / df[\"CNT_FAM_MEMBERS\"].replace(0, np.nan)\n",
    "    df[\"CHILDREN_TO_ADULT_RATIO\"] = df[\"CNT_CHILDREN\"] / (\n",
    "        df[\"CNT_FAM_MEMBERS\"] - df[\"CNT_CHILDREN\"] + 1\n",
    "    ).replace(0, np.nan)\n",
    "    df[\"LOW_SKILL_LABORER_FLAG\"] = np.where(\n",
    "        df[\"OCCUPATION_TYPE\"] == \"Low-skill Laborers\", 1, 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecca50be-81e4-4e98-af2c-7b62d09056bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bureau_features(df, active_status):\n",
    "\n",
    "    filtered_df = df[df[\"CREDIT_ACTIVE\"] == active_status]\n",
    "\n",
    "    agg_funcs = {\n",
    "        \"AMT_CREDIT_SUM\": [\"sum\", \"mean\", \"max\", \"min\"],\n",
    "        \"AMT_CREDIT_SUM_DEBT\": [\"sum\", \"mean\", \"max\", \"min\"],\n",
    "        \"AMT_CREDIT_SUM_OVERDUE\": [\"sum\", \"max\"],\n",
    "        \"AMT_CREDIT_MAX_OVERDUE\": [\"max\"],\n",
    "        \"CREDIT_DAY_OVERDUE\": [\"mean\"],\n",
    "        \"DAYS_CREDIT_ENDDATE\": [\"min\", \"max\"],\n",
    "        \"DAYS_ENDDATE_FACT\": [\"mean\"],\n",
    "        \"CNT_CREDIT_PROLONG\": [\"sum\"],\n",
    "        \"DAYS_CREDIT_UPDATE\": [\"mean\"],\n",
    "        \"AMT_ANNUITY\": [\"sum\", \"mean\"],\n",
    "    }\n",
    "\n",
    "    df_agg = filtered_df.groupby(\"SK_ID_CURR\").agg(agg_funcs).reset_index()\n",
    "\n",
    "    df_agg.columns = [\n",
    "        \"SK_ID_CURR\" if col[0] == \"SK_ID_CURR\" else f\"{col[0]}_{col[1]}\"\n",
    "        for col in df_agg.columns.values\n",
    "    ]\n",
    "\n",
    "    df_agg[\"CREDIT_ACTIVE_COUNT\"] = (\n",
    "        filtered_df.groupby(\"SK_ID_CURR\")[\"CREDIT_ACTIVE\"].count().values\n",
    "    )\n",
    "\n",
    "    df_agg[\"DEBT_TO_CREDIT_RATIO\"] = df_agg[\"AMT_CREDIT_SUM_DEBT_sum\"] / df_agg[\n",
    "        \"AMT_CREDIT_SUM_sum\"\n",
    "    ].replace(0, np.nan)\n",
    "    df_agg[\"CREDIT_UTILIZATION_RATIO\"] = df_agg[\"AMT_CREDIT_SUM_DEBT_sum\"] / df_agg[\n",
    "        \"AMT_CREDIT_SUM_sum\"\n",
    "    ].replace(0, np.nan)\n",
    "    df_agg[\"OVERDUE_RATIO\"] = df_agg[\"AMT_CREDIT_SUM_OVERDUE_sum\"] / df_agg[\n",
    "        \"AMT_CREDIT_SUM_sum\"\n",
    "    ].replace(0, np.nan)\n",
    "    df_agg[\"AVERAGE_LOAN_AMOUNT\"] = df_agg[\"AMT_CREDIT_SUM_sum\"] / df_agg[\n",
    "        \"CREDIT_ACTIVE_COUNT\"\n",
    "    ].replace(0, np.nan)\n",
    "    df_agg[\"PROLONGED_CREDIT_RATIO\"] = df_agg[\"CNT_CREDIT_PROLONG_sum\"] / df_agg[\n",
    "        \"CREDIT_ACTIVE_COUNT\"\n",
    "    ].replace(0, np.nan)\n",
    "    df_agg[\"ANNUITY_TO_CREDIT_RATIO\"] = df_agg[\"AMT_ANNUITY_sum\"] / df_agg[\n",
    "        \"AMT_CREDIT_SUM_sum\"\n",
    "    ].replace(0, np.nan)\n",
    "\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95bfafb6-5e6e-43ed-95c0-d8aa3d035b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_poscash_features(df):\n",
    "    pos_cash_agg = df.groupby(\"SK_ID_CURR\").agg(\n",
    "        {\n",
    "            \"SK_DPD\": [\"mean\", \"max\", \"min\"],\n",
    "            \"CNT_INSTALMENT_FUTURE\": [\"sum\", \"max\", \"min\", \"mean\"],\n",
    "            \"CNT_INSTALMENT\": \"mean\",\n",
    "            \"SK_ID_PREV\": \"nunique\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    pos_cash_agg.columns = [\n",
    "        \"SK_ID_CURR\" if col[0] == \"SK_ID_CURR\" else f\"{col[0]}_{col[1]}\"\n",
    "        for col in pos_cash_agg.columns.values\n",
    "    ]\n",
    "\n",
    "    return pos_cash_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce429a5f-6894-4787-8eb8-93dac2c9a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cc_balance_features(df):\n",
    "\n",
    "    df_agg = df.groupby(\"SK_ID_CURR\").agg(\n",
    "        {\n",
    "            \"SK_ID_PREV\": [\"count\", \"nunique\"],\n",
    "            \"MONTHS_BALANCE\": [\"mean\", \"max\", \"min\"],\n",
    "            \"AMT_BALANCE\": [\"mean\", \"sum\"],\n",
    "            \"AMT_CREDIT_LIMIT_ACTUAL\": [\"mean\", \"max\"],\n",
    "            \"AMT_DRAWINGS_ATM_CURRENT\": [\"sum\"],\n",
    "            \"AMT_DRAWINGS_CURRENT\": [\"sum\"],\n",
    "            \"AMT_DRAWINGS_OTHER_CURRENT\": [\"sum\"],\n",
    "            \"AMT_DRAWINGS_POS_CURRENT\": [\"sum\"],\n",
    "            \"AMT_INST_MIN_REGULARITY\": [\"mean\"],\n",
    "            \"AMT_PAYMENT_CURRENT\": [\"sum\"],\n",
    "            \"AMT_PAYMENT_TOTAL_CURRENT\": [\"sum\"],\n",
    "            \"AMT_RECEIVABLE_PRINCIPAL\": [\"sum\"],\n",
    "            \"AMT_RECIVABLE\": [\"sum\"],\n",
    "            \"AMT_TOTAL_RECEIVABLE\": [\"sum\"],\n",
    "            \"CNT_DRAWINGS_ATM_CURRENT\": [\"sum\"],\n",
    "            \"CNT_DRAWINGS_CURRENT\": [\"sum\"],\n",
    "            \"CNT_DRAWINGS_OTHER_CURRENT\": [\"sum\"],\n",
    "            \"CNT_DRAWINGS_POS_CURRENT\": [\"sum\"],\n",
    "            \"CNT_INSTALMENT_MATURE_CUM\": [\"sum\"],\n",
    "            \"SK_DPD\": [\"mean\", \"max\"],\n",
    "            \"SK_DPD_DEF\": [\"mean\", \"max\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_agg.columns = [\"_\".join(col).strip().upper() for col in df_agg.columns]\n",
    "    df_agg.rename(columns={\"SK_ID_CURR_\": \"SK_ID_CURR\"}, inplace=True)\n",
    "\n",
    "    df_agg[\"AVG_PAYMENT_TO_BALANCE_RATIO\"] = df_agg[\n",
    "        \"AMT_PAYMENT_TOTAL_CURRENT_SUM\"\n",
    "    ] / df_agg[\"AMT_BALANCE_MEAN\"].replace(0, np.nan)\n",
    "    df_agg[\"TOTAL_DRAWINGS_RATIO\"] = (\n",
    "        df_agg[\"AMT_DRAWINGS_ATM_CURRENT_SUM\"]\n",
    "        + df_agg[\"AMT_DRAWINGS_CURRENT_SUM\"]\n",
    "        + df_agg[\"AMT_DRAWINGS_OTHER_CURRENT_SUM\"]\n",
    "        + df_agg[\"AMT_DRAWINGS_POS_CURRENT_SUM\"]\n",
    "    ) / df_agg[\"AMT_CREDIT_LIMIT_ACTUAL_MEAN\"].replace(0, np.nan)\n",
    "    df_agg[\"OVERDUE_RATIO\"] = df_agg[\"SK_DPD_DEF_MAX\"] / (\n",
    "        df_agg[\"SK_DPD_DEF_MAX\"] + df_agg[\"SK_DPD_MAX\"]\n",
    "    ).replace(0, np.nan)\n",
    "\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb559668-627a-4e42-8d6d-db10d5e4fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_installments_features(df):\n",
    "\n",
    "    df[\"DAYS_LATE\"] = df[\"DAYS_ENTRY_PAYMENT\"] - df[\"DAYS_INSTALMENT\"]\n",
    "    df[\"PAYMENT_RATIO\"] = df[\"AMT_PAYMENT\"] / df[\"AMT_INSTALMENT\"].replace(0, 1)\n",
    "\n",
    "    df_agg = (\n",
    "        df.groupby(\"SK_ID_CURR\")\n",
    "        .agg(\n",
    "            Average_Installment=(\"AMT_INSTALMENT\", \"mean\"),\n",
    "            Total_Payments=(\"AMT_PAYMENT\", \"sum\"),\n",
    "            Payment_Ratio=(\"PAYMENT_RATIO\", \"mean\"),\n",
    "            Average_Days_Late=(\"DAYS_LATE\", \"mean\"),\n",
    "            Total_Installments=(\"NUM_INSTALMENT_NUMBER\", \"count\"),\n",
    "            Max_Payment=(\"AMT_PAYMENT\", \"max\"),\n",
    "            Min_Payment=(\"AMT_PAYMENT\", \"min\"),\n",
    "            Late_Payment_Count=(\"DAYS_LATE\", lambda x: (x > 0).sum()),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff99fa8e-b75b-41be-9ad1-012246260501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prev_features(df):\n",
    "    df[\"APP_CREDIT_PERC\"] = df[\"AMT_APPLICATION\"] / df[\"AMT_CREDIT\"]\n",
    "    df[\"AMT_DIFF_CREAPP\"] = df[\"AMT_APPLICATION\"] - df[\"AMT_CREDIT\"]\n",
    "    df[\"AMT_DIFF_CREDIT_GOODS\"] = df[\"AMT_CREDIT\"] - df[\"AMT_GOODS_PRICE\"]\n",
    "    df[\"AMT_CREDIT_GOODS_PERC\"] = df[\"AMT_CREDIT\"] / df[\"AMT_GOODS_PRICE\"].replace(\n",
    "        0, np.nan\n",
    "    )\n",
    "    df[\"AMT_PAY_YEAR\"] = df[\"AMT_CREDIT\"] / df[\"AMT_ANNUITY\"].replace(0, np.nan)\n",
    "    df[\"DAYS_TOTAL\"] = df[\"DAYS_LAST_DUE\"] - df[\"DAYS_FIRST_DUE\"]\n",
    "    df[\"DAYS_TOTAL2\"] = df[\"DAYS_LAST_DUE_1ST_VERSION\"] - df[\"DAYS_FIRST_DUE\"]\n",
    "    df[\"DAYS_END_DIFF\"] = df[\"DAYS_LAST_DUE_1ST_VERSION\"] - df[\"DAYS_LAST_DUE\"]\n",
    "\n",
    "    num_aggregations = {\n",
    "        \"SK_ID_PREV\": [\"count\"],\n",
    "        \"AMT_ANNUITY\": [\"max\", \"mean\"],\n",
    "        \"AMT_APPLICATION\": [\"max\", \"mean\"],\n",
    "        \"AMT_CREDIT\": [\"mean\", \"sum\"],\n",
    "        \"APP_CREDIT_PERC\": [\"max\", \"mean\"],\n",
    "        \"AMT_DIFF_CREAPP\": [\"max\", \"mean\"],\n",
    "        \"AMT_DIFF_CREDIT_GOODS\": [\"max\", \"mean\"],\n",
    "        \"AMT_CREDIT_GOODS_PERC\": [\"max\", \"mean\"],\n",
    "        \"AMT_PAY_YEAR\": [\"max\", \"mean\"],\n",
    "        \"AMT_DOWN_PAYMENT\": [\"max\", \"mean\"],\n",
    "        \"RATE_DOWN_PAYMENT\": [\"max\", \"mean\"],\n",
    "        \"DAYS_DECISION\": [\"max\", \"mean\", \"min\"],\n",
    "        \"CNT_PAYMENT\": [\"mean\", \"sum\"],\n",
    "    }\n",
    "\n",
    "    df_agg = df.groupby(\"SK_ID_CURR\").agg(num_aggregations).reset_index()\n",
    "\n",
    "    df_agg.columns = [\n",
    "        \"SK_ID_CURR\" if col[0] == \"SK_ID_CURR\" else f\"{col[0]}_{col[1]}\"\n",
    "        for col in df_agg.columns.values\n",
    "    ]\n",
    "\n",
    "    return df_agg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
